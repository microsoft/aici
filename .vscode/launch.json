{
    "version": "0.2.0",
    "configurations": [
        {
            "type": "lldb",
            "request": "launch",
            "name": "rllm-llamacpp phi",
            "cwd": "rllm/rllm-llamacpp",
            "preLaunchTask": "rllm-llamacpp: build",
            "program": "${workspaceFolder}/target/debug/rllm-llamacpp",
            "env": {
                "RUST_LOG": "info,tokenizers=error,rllm=trace,aicirt=info,llama_cpp_low=trace"
            },
            "args": [
                "--verbose",
                "--aicirt=${workspaceFolder}/target/release/aicirt",
                "--model=https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q8_0.gguf",
                "--gpu-layers=100"
            ]
        },
        {
            "name": "vllm.entrypoints.openai.api_server",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/py/vllm/vllm/entrypoints/openai/api_server.py",
            "env": {
                "RUST_LOG": "info,tokenizers=error,aicirt=info",
                "RUST_BACKTRACE": "1",
                "PYTHONPATH": "py:py/vllm"
            },
            "args": [
                "--enforce-eager",
                "--use-v2-block-manager",
                "--enable-chunked-prefill",
                "--aici-rt",
                "${workspaceFolder}/target/release/aicirt",
                "-A--wasm-timer-resolution-us=10",
                "--model", "microsoft/Orca-2-13b",
                "--revision", "refs/pr/22",
                "--aici-tokenizer", "orca",
                "--port", "4242",
                "--host", "127.0.0.1"
            ],
        }
    ]
}