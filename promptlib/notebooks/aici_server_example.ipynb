{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is demonstrating (and testing) scenarios for generating requests to the AICI server (evolving API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\prompt-plan\\\\promptlib\\\\')\n",
    "sys.path.insert(0, '/workspaces/aici/promptlib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from promptlib import PromptNode\n",
    "from promptlib import set_model, append, gen, choose, begin, end\n",
    "from promptlib.models import LLM, TransformersLLM\n",
    "from promptlib.endpoints import AICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are examples of plans generated by promptlib\n",
    "\n",
    "First, we show building a single path through a prompt tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': [{'Model': {'model': 'gpt2'}}, {'Fixed': {'text': 'Here is a generated prompt:', 'tag': 'prompt1'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}, {'Fixed': {'text': 'Here is another generated prompt. '}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}, 'mask_tags': ['prompt1']}}]}\n"
     ]
    }
   ],
   "source": [
    "llm = TransformersLLM(\"gpt2\")\n",
    "\n",
    "pn = PromptNode().set_model(llm)                                          \\\n",
    "    .append(\"Here is a generated prompt:\", tag=\"prompt1\").gen(max_tokens=10)             \\\n",
    "    .append(\"Here is another generated prompt. \").gen(max_tokens=10, ignore=[\"prompt1\"])\n",
    "\n",
    "aici_steps = pn.build_linear_plan()\n",
    "print(aici_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': [{'Model': {'model': 'gpt2'}}, {'block': {'steps': [{'Fixed': {'text': 'Here is a generated prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}, {'Fixed': {'text': 'Here is another generated prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}], 'id': 'block1'}}, {'Fixed': {'text': 'Here is a final prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}]}\n"
     ]
    }
   ],
   "source": [
    "llm = TransformersLLM(\"gpt2\")\n",
    "\n",
    "pn0  = PromptNode()\n",
    "\n",
    "pn = pn0.set_model(llm)                                          \\\n",
    "    .begin(id=\"block1\") \\\n",
    "    .append(\"Here is a generated prompt. Resp:\").gen(max_tokens=10)             \\\n",
    "    .append(\"Here is another generated prompt. Resp:\").gen(max_tokens=10)      \\\n",
    "    .end() \\\n",
    "    .append(\"Here is a final prompt. Resp:\").gen(max_tokens=10)  \n",
    "    \n",
    "\n",
    "aici_steps = pn0.build_tree_plan()\n",
    "print(aici_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraints we are imposing here have both a python code implementation and also return a regex that can impose the same constraint.\n",
    "\n",
    "The general idea is that a constraint in python has to match some implementation of a constraint in the Rust/WASM AST-Runner.  If none exists, then we need to upload imperative code (in WASM) to the server to implement the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': [{'Model': {'model': 'gpt2'}}, {'block': {'steps': [{'constraint': {'type': 'regex', 'rx': '[h|e|l|o]+'}}, {'constraint': {'type': 'regex', 'rx': '^(?:[13579][02468])*[13579]|[02468]([13579]?|(?:[13579][02468])*)$'}}, {'Fixed': {'text': 'Here is a generated prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}, {'Fixed': {'text': 'Here is another generated prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}], 'id': 'block1'}}, {'Fixed': {'text': 'Here is a final prompt. Resp:'}}, {'Gen': {'max_tokens': 10, 'rx': '.+', 'genargs': {}}}]}\n"
     ]
    }
   ],
   "source": [
    "llm = TransformersLLM(\"gpt2\")\n",
    "\n",
    "from promptlib.constraints import DummyCharacterConstraint, OddEvenDigitConstraint\n",
    "\n",
    "pn0  = PromptNode()\n",
    "\n",
    "pn = pn0.set_model(llm)                                          \\\n",
    "    .begin(id=\"block1\") \\\n",
    "    .constrain(DummyCharacterConstraint(['h','e','l','o'])) \\\n",
    "    .constrain(OddEvenDigitConstraint()) \\\n",
    "    .append(\"Here is a generated prompt. Resp:\").gen(max_tokens=10)             \\\n",
    "    .append(\"Here is another generated prompt. Resp:\").gen(max_tokens=10)      \\\n",
    "    .end() \\\n",
    "    .append(\"Here is a final prompt. Resp:\").gen(max_tokens=10)  \n",
    "    \n",
    "\n",
    "aici_steps = pn0.build_tree_plan()\n",
    "print(aici_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of actually uploading a program and plan to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 3270kB -> 10168kB id:292267c9\n",
      "Please answer the following questions:\n",
      " Q: Who is the president of the USA?\n",
      " A: The president of the USA is Donald Trump.\n",
      "Q: And who is the vice president?\n",
      " A: The vice president of the USA is Mike Pence.\n",
      "Please give a url with evidence\n",
      " http://www.whitehouse.gov/\n",
      "http://www.whitehouse.gov/president/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Please answer the following questions:\\n Q: Who is the president of the USA?\\n A: The president of the USA is Donald Trump.\\nQ: And who is the vice president?\\n A: The vice president of the USA is Mike Pence.\\nPlease give a url with evidence\\n http://www.whitehouse.gov/\\nhttp://www.whitehouse.gov/president/'],\n",
       " {'model': '',\n",
       "  'prompt': '',\n",
       "  'max_tokens': 200,\n",
       "  'n': 1,\n",
       "  'temperature': 0,\n",
       "  'stream': True,\n",
       "  'aici_module': '292267c9153c77780d465413dcaae79a5f0f4f37736af247f906381c368d8cfe',\n",
       "  'aici_arg': {'steps': [{'Fixed': {'text': {'String': {'str': 'Please answer the following questions:\\n Q: Who is the president of the USA?\\n A:'}},\n",
       "      'tag': '_2',\n",
       "      'label': '_2'}},\n",
       "    {'Gen': {'max_tokens': 10, 'tag': '_2', 'label': '_2'}},\n",
       "    {'Fixed': {'text': {'String': {'str': '\\nQ: And who is the vice president?\\n A:'}},\n",
       "      'tag': '_3',\n",
       "      'label': '_3'}},\n",
       "    {'Gen': {'max_tokens': 20, 'tag': '_3', 'label': '_3'}},\n",
       "    {'Fixed': {'text': {'String': {'str': '\\nPlease give a url with evidence\\n http://'}},\n",
       "      'tag': '_4',\n",
       "      'label': '_4'}},\n",
       "    {'Gen': {'max_tokens': 20, 'tag': '_4', 'label': '_4'}}]}},\n",
       " [{'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Please',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'dfa: 144 bytes\\ndfa: 144 bytes\\ndfa: 144 bytes\\n[0] Fixed(\"Please answer the following questions:\\\\n Q: Who is the president of the USA?\\\\n A:\", tag:TagName(\"_2\"), label:_2) [Fixed()] tok:0/inf\\n[1] Gen(max_tokens:10, , tag:TagName(\"_2\"), label:_2) [Gen()] tok:0/10\\n[2] Fixed(\"\\\\nQ: And who is the vice president?\\\\n A:\", tag:TagName(\"_3\"), label:_3) [Fixed()] tok:0/inf\\n[3] Gen(max_tokens:20, , tag:TagName(\"_3\"), label:_3) [Gen()] tok:0/20\\n[4] Fixed(\"\\\\nPlease give a url with evidence\\\\n http://\", tag:TagName(\"_4\"), label:_4) [Fixed()] tok:0/inf\\n[5] Gen(max_tokens:20, , tag:TagName(\"_4\"), label:_4) [Gen()] tok:0/20\\n[6] Stop [Stop] tok:0/inf\\nprompt: [1]\\ntokenize_bytes: \"Please answer the following questions:\\\\n Q: Who is the president of the USA?\\\\n A:\" -> [12148, 1234, 278, 1494, 5155, 29901, 13, 660, 29901, 11644, 338, 278, 6673, 310, 278, 8278, 29973, 13, 319, 29901]\\nfinish: [Fixed()] tok:20/inf \"Please answer the following questions:\\\\n Q: Who is the president of the USA?\\\\n A:\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.18}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' answer the following questions:\\n Q: Who is the president of the USA?\\n A: The',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' president',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'tokenize_bytes: \"\\\\nQ: And who is the vice president?\\\\n A:\" -> [13, 29984, 29901, 1126, 1058, 338, 278, 11289, 6673, 29973, 13, 319, 29901]\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.34}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' of',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' the',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' USA',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' is',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Donald',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Trump',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'finish: [Gen()] tok:9/10 \"The president of the USA is Donald Trump.\\\\n\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Q',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'finish: [Fixed()] tok:13/inf \"\\\\nQ: And who is the vice president?\\\\n A:\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.02}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ': And who is the vice president?\\n A: The',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' vice',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'tokenize_bytes: \"\\\\nPlease give a url with evidence\\\\n http://\" -> [13, 12148, 2367, 263, 3142, 411, 10757, 13, 1732, 597]\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.34}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' president',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' of',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' the',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' USA',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.29}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' is',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Mike',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' P',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'ence',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'finish: [Gen()] tok:11/20 \"The vice president of the USA is Mike Pence.\\\\n\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Please',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'finish: [Fixed()] tok:10/inf \"\\\\nPlease give a url with evidence\\\\n http://\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.02}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' give a url with evidence\\n http://www',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'white',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'house',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'gov',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '/',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'http',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '://',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'www',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'white',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'house',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.28}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '.',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.29}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'gov',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '/',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'pres',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'ident',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.27}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '/',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'finish: [Gen()] tok:20/20 \"www.whitehouse.gov/\\\\nhttp://www.whitehouse.gov/president/\"\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.30000000000000004}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'stop: no state allows token\\n',\n",
       "     'storage': [],\n",
       "     'millis': 0.04}]},\n",
       "  {'id': 'cmpl-09c74d8ad6fc4fd79295e908f39a04bc',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701330062,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'storage': [],\n",
       "     'millis': 0.0}]}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"Please answer the following questions:\\n Q: Who is the president of the USA?\\n A:\").gen(max_tokens=10)             \\\n",
    "    .append(\"\\nQ: And who is the vice president?\\n A:\", attrs=[\"allcaps\"]).gen(max_tokens=20) \\\n",
    "    .append(\"\\nPlease give a url with evidence\\n http://\").gen(max_tokens=20)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 3270kB -> 10168kB id:292267c9\n",
      "{'steps': [{'Fixed': {'text': {'String': {'str': 'Please answer the following questions:\\n Q: Who is the president of the USA?\\n A:'}}, 'tag': '_2', 'label': '_2'}}, {'Gen': {'max_tokens': 10, 'tag': '_2', 'label': '_2'}}, {'Fixed': {'text': {'String': {'str': '\\nQ: And who is the vice president?\\n A:'}}, 'tag': '_3', 'label': '_3'}}, {'Gen': {'max_tokens': 20, 'tag': '_3', 'label': '_3'}}, {'Fixed': {'text': {'String': {'str': '\\nPlease give a url with evidence\\n http://'}}, 'tag': '_4', 'label': '_4'}}, {'Fork': {'branches': [[{'Fixed': {'following': '_3', 'text': {'Concat': {'parts': [{'String': {'str': '{{_3}}'}}, {'String': {'str': '\\nPlease give a url with evidence\\n http://'}}]}}}}, {'Gen': {'max_tokens': 20, 'tag': '_4', 'label': '_4', 'stmts': [{'Set': {'var': 'gen_var_4', 'expr': {'Current': {}}}}]}}], [{'Wait': {'vars': ['gen_var_4']}}, {'Fixed': {'text': {'Var': {'var': 'gen_var_4'}}}}]]}}]}\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidChunkLength\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1073\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1074\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1008\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m \u001b[39mraise\u001b[39;00m InvalidChunkLength(\u001b[39mself\u001b[39m, line) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidChunkLength\u001b[0m: InvalidChunkLength(got length b'', 0 bytes read)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1061\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[39mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[1;32m   1057\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[0;32m-> 1061\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m   1062\u001b[0m     \u001b[39m# Don't bother reading the body of a HEAD request.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response \u001b[39mand\u001b[39;00m is_response_to_head(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response):\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:727\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mexcept\u001b[39;00m (HTTPException, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    726\u001b[0m     \u001b[39m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnection broken: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[39m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m steps \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39mbuild_tree_plan()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(steps)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m endpoint\u001b[39m.\u001b[39;49mrun(pn)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoint.py:17\u001b[0m, in \u001b[0;36mEndpointNode.run\u001b[0;34m(self, promptNode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, promptNode:PromptNode):\n\u001b[1;32m     16\u001b[0m     plan \u001b[39m=\u001b[39m promptNode\u001b[39m.\u001b[39mbuild_linear_plan()\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint\u001b[39m.\u001b[39;49mrun(plan)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoints/aici.py:27\u001b[0m, in \u001b[0;36mAICI.run\u001b[0;34m(self, prompt_plan)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, prompt_plan):\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m _submit_program(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_url, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwasm_runner_id, prompt_plan, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoints/aici.py:80\u001b[0m, in \u001b[0;36m_submit_program\u001b[0;34m(base_url, aici_module, aici_arg, temperature, max_tokens, n, log)\u001b[0m\n\u001b[1;32m     78\u001b[0m full_resp \u001b[39m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m texts \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m n\n\u001b[0;32m---> 80\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39miter_lines():\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m line:\n\u001b[1;32m     82\u001b[0m         decoded_line: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, decode_unicode\u001b[39m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m pending \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39m pending \u001b[39m+\u001b[39m chunk\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    819\u001b[0m \u001b[39mexcept\u001b[39;00m DecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"Please answer the following questions:\\n Q: Who is the president of the USA?\\n A:\").gen(max_tokens=10)             \\\n",
    "    .append(\"\\nQ: And who is the vice president?\\n A:\", attrs=[\"allcaps\"]).gen(max_tokens=20) \\\n",
    "    .append(\"\\nPlease give a url with evidence\\n http://\").gen(max_tokens=20, ignore=[\"allcaps\"])\n",
    "\n",
    "steps = endpoint.build_tree_plan()\n",
    "print(steps)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 3185kB -> 9853kB id:f46c460b\n",
      "{'steps': [{'Fixed': {'text': ' Please answer the following questions'}}, {'Fixed': {'text': '(And answer in ALL CAPS):', 'tag': 'allcaps'}}, {'Fixed': {'text': '(and answer in all lowercase)', 'tag': 'alllower'}}, {'Fixed': {'text': '\\nQ: Why did Einstein say he liked to play violin?\\n A:'}}, {'Gen': {'max_tokens': 30, 'genargs': {}, 'mask_tags': ['allcaps']}}, {'Fixed': {'text': '\\nQ: Who is the president of the USA?\\n A:'}}, {'Gen': {'max_tokens': 30, 'genargs': {}, 'mask_tags': ['alllower']}}, {'Fixed': {'text': '\\nQ: And who is the vice president?\\n A:'}}, {'Gen': {'max_tokens': 20, 'genargs': {}}}, {'Fixed': {'text': '\\nPlease give a url with evidence\\n http://'}}, {'Gen': {'max_tokens': 20, 'genargs': {}}}]}\n",
      "Please answer the following questions(And answer in ALL CAPS):(and answer in all lowercase)\n",
      "Q: Why did Einstein say he liked to play violin?\n",
      " A: Because he was a little out of tune.\n",
      "Q: Who is the president of the USA?\n",
      " A: I don't know.\n",
      "Q: And who is the vice president?\n",
      " A: I don't know.\n",
      "Please give a url with evidence\n",
      " http://www.youtube.com/watch?v=9999999999\n"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\" Please answer the following questions\") \\\n",
    "    .append(\"(And answer in ALL CAPS):\", tag=\"allcaps\") \\\n",
    "    .append(\"(and answer in all lowercase)\", tag=\"alllower\") \\\n",
    "    .append(\"\\nQ: Why did Einstein say he liked to play violin?\\n A:\").gen(max_tokens=30, ignore=[\"allcaps\"])             \\\n",
    "    .append(\"\\nQ: Who is the president of the USA?\\n A:\").gen(max_tokens=30, ignore=[\"alllower\"])             \\\n",
    "    .append(\"\\nQ: And who is the vice president?\\n A:\").gen(max_tokens=20) \\\n",
    "    .append(\"\\nPlease give a url with evidence\\n http://\").gen(max_tokens=20)\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\" Please answer the following questions\") \\\n",
    "    .append(\"(And answer in ALL CAPS):\", tag=\"allcaps\") \\\n",
    "    .append(\"\\nQ: Why did Einstein say he liked to play violin?\\n A:\").gen(max_tokens=30)             \\\n",
    "    .append(\"\\nQ: Who is the president of the USA?\\n A:\").gen(max_tokens=30, ignore=[\"allcaps\"])             \\\n",
    "    .append(\"\\nQ: And who is the vice president?\\n A:\").gen(max_tokens=20) \\\n",
    "    .append(\"\\nPlease give a url with evidence\\n http://\").gen(max_tokens=20)\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 2688kB -> 7999kB id:6abb139f\n",
      "{'model': '', 'prompt': '', 'max_tokens': 200, 'n': 1, 'temperature': 0, 'stream': True, 'aici_module': '6abb139fdd3b596815e342dc3bb124a8cc4fbc560a75f3e94965f0e8954957e1', 'aici_arg': {'steps': [{'Fixed': {'text': 'Please answer the following questions'}}, {'Fixed': {'text': '\\nQ: Why did Einstein say he liked to play violin?\\n'}}, {'Fixed': {'text': '\\n(And always answer in ALL CAPS)', 'tag': 'allcaps'}}, {'Fixed': {'text': '\\nA:'}}, {'Gen': {'max_tokens': 30, 'genargs': {}}}]}}\n",
      "Please answer the following questions\n",
      "Q: Why did Einstein say he liked to play violin?\n",
      "\n",
      "(And always answer in ALL CAPS)\n",
      "A: BECAUSE HE WAS A VIOLINIST\n",
      "\n",
      "Comment: I'm voting to close this question as off-topic because it is\n"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"Please answer the following questions\") \\\n",
    "    .append(\"\\nQ: Why did Einstein say he liked to play violin?\\n\") \\\n",
    "    .append(\"\\n(And always answer in ALL CAPS)\", tag=\"allcaps\") \\\n",
    "    .append(\"\\nA:\") \\\n",
    "    .gen(max_tokens=30) #, ignore=[\"allcaps\"])\n",
    "\n",
    "#aici_steps = endpoint.build_tree_plan()\n",
    "#print(aici_steps)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 2688kB -> 7999kB id:6abb139f\n",
      "{'model': '', 'prompt': '', 'max_tokens': 200, 'n': 1, 'temperature': 0, 'stream': True, 'aici_module': '6abb139fdd3b596815e342dc3bb124a8cc4fbc560a75f3e94965f0e8954957e1', 'aici_arg': {'steps': [{'Fixed': {'text': 'Please repeat the following list in order:'}}, {'Fixed': {'text': '\\nApple'}}, {'Fixed': {'text': '\\nCherries', 'tag': 'selected'}}, {'Fixed': {'text': '\\nGrapes', 'tag': 'selected'}}, {'Fixed': {'text': '\\nStrawberries'}}, {'Fixed': {'text': '\\nBananas', 'tag': 'selected'}}, {'Fixed': {'text': '\\n\\nOk now please repeat the list:\\n'}}, {'Gen': {'max_tokens': 30, 'genargs': {}, 'mask_tags': ['selected']}}]}}\n",
      "Please repeat the following list in order:\n",
      "Apple\n",
      "Cherries\n",
      "Grapes\n",
      "Strawberries\n",
      "Bananas\n",
      "\n",
      "Ok now please repeat the list:\n",
      "Apple\n",
      "Banana\n",
      "Orange\n",
      "Pear\n",
      "\n",
      "Please repeat the following list in order:\n",
      "Apple\n",
      "Banana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"Please repeat the following list in order:\") \\\n",
    "    .append(\"\\nApple\") \\\n",
    "    .append(\"\\nCherries\", tag=\"selected\") \\\n",
    "    .append(\"\\nGrapes\", tag=\"selected\") \\\n",
    "    .append(\"\\nStrawberries\") \\\n",
    "    .append(\"\\nBananas\", tag=\"selected\") \\\n",
    "    .append(\"\\n\\nOk now please repeat the list:\\n\") \\\n",
    "    .gen(max_tokens=30, ignore=[\"selected\"])\n",
    "\n",
    "#aici_steps = endpoint.build_tree_plan()\n",
    "#print(aici_steps)\n",
    "\n",
    "endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AICI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ep \u001b[39m=\u001b[39m AICI(base_url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp://127.0.0.1:8080/v1/\u001b[39m\u001b[39m\"\u001b[39m, wasm_runner_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/workspaces/aici/aici_ast_runner/target/opt.wasm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m endpoint \u001b[39m=\u001b[39m PromptNode()\u001b[39m.\u001b[39mset_endpoint(ep)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pn \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m[INST] Please repeat the following list in order:\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mApple\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCherries\u001b[39m\u001b[39m\"\u001b[39m, attrs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mselected\u001b[39m\u001b[39m\"\u001b[39m]) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mOk now please repeat the list and say DONE when DONE:[/INST]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mgen(max_tokens\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, stop_at\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDONE\u001b[39m\u001b[39m\"\u001b[39m, ignore\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mselected\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AICI' is not defined"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"[INST] Please repeat the following list in order:\") \\\n",
    "    .append(\"\\nApple\") \\\n",
    "    .append(\"\\nCherries\", attrs=[\"selected\"]) \\\n",
    "    .append(\"\\nGrapes\", attrs=[\"selected\"]) \\\n",
    "    .append(\"\\nStrawberries\") \\\n",
    "    .append(\"\\nBananas\", attrs=[\"selected\"]) \\\n",
    "    .append(\"\\n\\nOk now please repeat the list and say DONE when DONE:[/INST]\\n\") \\\n",
    "    .gen(max_tokens=30, stop_at=\"DONE\", ignore=[\"selected\"])\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "results = endpoint.runAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please repeat the following list in order:\n",
      "Apple\n",
      "Cherries\n",
      "Grapes\n",
      "Strawberries\n",
      "Bananas\n",
      "\n",
      "Strawberries\n",
      "\n",
      "Ok now please repeat the list:\n",
      "Apple\n",
      "Strawberries\n",
      "\n",
      "Please repeat the following list in order:\n",
      "Apple\n",
      "Strawberries\n",
      "\n",
      "Ok now please\n"
     ]
    }
   ],
   "source": [
    "print(results[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 3185kB -> 9853kB id:f46c460b\n",
      "{'steps': [{'Fixed': {'text': 'Please remember the following words: Apple, Bananas, and Telephone\\n', 'expand_vars': True, 'tag': '_2', 'label': '_2'}}, {'Fork': {'branches': [[{'Fixed': {'text': '\\nNow repeat those words in French: ', 'expand_vars': True, 'tag': '_3', 'label': '_3'}}, {'Gen': {'max_tokens': 30, 'tag': '_2', 'label': '_2', 'set_var': 'french'}}], [{'Fixed': {'text': '\\nNow repeat those words in German: ', 'expand_vars': True, 'tag': '_4', 'label': '_4'}}, {'Gen': {'max_tokens': 30, 'tag': '_3', 'label': '_3', 'set_var': 'german'}}], [{'Wait': {'vars': ['french', 'german'], 'tag': '_2', 'label': '_2'}}, {'Fixed': {'text': '\\nIn german {{german}} means {{french}} in french', 'expand_vars': True, 'tag': '_5', 'label': '_5'}}]]}}]}\n",
      "['Please remember the following words: Apple, Bananas, and Telephone\\n\\nNow repeat those words in French: \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish', \"Please remember the following words: Apple, Bananas, and Telephone\\n\\nNow repeat those words in German: \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let's try\", \"Please remember the following words: Apple, Bananas, and Telephone\\n\\nIn german \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let's try means \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish in french\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Please remember the following words: Apple, Bananas, and Telephone\\n\\nNow repeat those words in French: \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish',\n",
       "  \"Please remember the following words: Apple, Bananas, and Telephone\\n\\nNow repeat those words in German: \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let's try\",\n",
       "  \"Please remember the following words: Apple, Bananas, and Telephone\\n\\nIn german \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let's try means \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish in french\"],\n",
       " {'model': '',\n",
       "  'prompt': '',\n",
       "  'max_tokens': 200,\n",
       "  'n': 1,\n",
       "  'temperature': 0,\n",
       "  'stream': True,\n",
       "  'aici_module': 'f46c460b5c4a5de00e4d26ad04a421136d2b4257334b046a826d1e1dccad5236',\n",
       "  'aici_arg': {'steps': [{'Fixed': {'text': 'Please remember the following words: Apple, Bananas, and Telephone\\n',\n",
       "      'expand_vars': True,\n",
       "      'tag': '_2',\n",
       "      'label': '_2'}},\n",
       "    {'Fork': {'branches': [[{'Fixed': {'text': '\\nNow repeat those words in French: ',\n",
       "          'expand_vars': True,\n",
       "          'tag': '_3',\n",
       "          'label': '_3'}},\n",
       "        {'Gen': {'max_tokens': 30,\n",
       "          'tag': '_2',\n",
       "          'label': '_2',\n",
       "          'set_var': 'french'}}],\n",
       "       [{'Fixed': {'text': '\\nNow repeat those words in German: ',\n",
       "          'expand_vars': True,\n",
       "          'tag': '_4',\n",
       "          'label': '_4'}},\n",
       "        {'Gen': {'max_tokens': 30,\n",
       "          'tag': '_3',\n",
       "          'label': '_3',\n",
       "          'set_var': 'german'}}],\n",
       "       [{'Wait': {'vars': ['french', 'german'], 'tag': '_2', 'label': '_2'}},\n",
       "        {'Fixed': {'text': '\\nIn german {{german}} means {{french}} in french',\n",
       "          'expand_vars': True,\n",
       "          'tag': '_5',\n",
       "          'label': '_5'}}]]}}]}},\n",
       " [{'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Please',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'dfa: 144 bytes\\ndfa: 144 bytes\\n[0] Fixed(f\"Please remember the following words: Apple, Bananas, and Telephone\\\\n\", tag:TagName(\"_2\")) [Fixed(f\"Please remember the following ...\")] tok:0/inf\\n[1] Fork {  branch:\\n      Fixed(f\"\\\\nNow repeat those words in French: \", tag:TagName(\"_3\"))\\n      Gen(max_tokens:30, , $french := out, tag:TagName(\"_2\"))\\n  branch:\\n      Fixed(f\"\\\\nNow repeat those words in German: \", tag:TagName(\"_4\"))\\n      Gen(max_tokens:30, , $german := out, tag:TagName(\"_3\"))\\n  branch:\\n      Wait([$french, $german])\\n      Fixed(f\"\\\\nIn german {{german}} means {{french}} in french\", tag:TagName(\"_5\"))\\n} [Fork(3)] tok:0/inf\\n[2] Stop [Stop] tok:0/inf\\nprompt: [1]\\nexp: Please remember the following words: Apple, Bananas, and Telephone\\n -> Please remember the following words: Apple, Bananas, and Telephone\\n\\ntokenize: \"Please remember the following words: Apple, Bananas, and Telephone\\\\n\" -> [12148, 6456, 278, 1494, 3838, 29901, 12113, 29892, 10765, 16397, 29892, 322, 9699, 6710, 13]\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' remember the following words: Apple, Bananas, and Telephone\\n\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'fork group: [SeqId(50), SeqId(51), SeqId(52)]\\nexp: \\nNow repeat those words in French:  -> \\nNow repeat those words in French: \\ntokenize: \"\\\\nNow repeat those words in French: \" -> [13, 10454, 12312, 1906, 3838, 297, 5176, 29901, 29871]\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'Please remember the following words: Apple, Bananas, and Telephone\\n\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'fork group: [SeqId(50), SeqId(51), SeqId(52)]\\nexp: \\nNow repeat those words in German:  -> \\nNow repeat those words in German: \\ntokenize: \"\\\\nNow repeat those words in German: \" -> [13, 10454, 12312, 1906, 3838, 297, 5332, 29901, 29871]\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': 'Please remember the following words: Apple, Bananas, and Telephone\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'fork group: [SeqId(50), SeqId(51), SeqId(52)]\\nwait [$french, $german] suspend\\ntokenize: \"\" -> [30833]\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'Now repeat those words in German: \\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Now repeat those words in French: \\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'App',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'App',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'le',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'le',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ' Ap',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Pom',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'me',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'fel',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'B',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'B',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'an',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'an',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'anas',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'anas',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ' Ban',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Ban',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'anes',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'an',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'en',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'T',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'ele',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'T',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'phone',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'ele',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'phone',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' T',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ':',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'l',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ' Tele',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'phone',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'fon',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': 'Now',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 'Now',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' repeat',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ',',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' those',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ' let',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' words',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': \"'\",\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' in',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': 's',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] suspend\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': ' Spanish',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': ' try',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '\\n',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': 'wait [$french, $german] done\\nfinish: [Wait(2)] tok:0/inf \"\"\\nexp: \\nIn german {{german}} means {{french}} in french -> \\nIn german \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let\\'s try means \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish in french\\ntokenize: \"\\\\nIn german \\\\n\\\\nApple: Apfel\\\\nBananas: Bananen\\\\nTelephone: Telefon\\\\n\\\\nNow, let\\'s try means \\\\n\\\\nApple: Pomme\\\\nBananas: Bananes\\\\nTelephone: Tlphone\\\\n\\\\nNow repeat those words in Spanish in french\" -> [13, 797, 330, 3504, 29871, 13, 13, 2052, 280, 29901, 6225, 13287, 13, 29933, 273, 16397, 29901, 10765, 273, 264, 13, 29911, 6146, 6710, 29901, 9699, 18753, 13, 13, 10454, 29892, 1235, 29915, 29879, 1018, 2794, 29871, 13, 13, 2052, 280, 29901, 14351, 1004, 13, 29933, 273, 16397, 29901, 10765, 17297, 13, 29911, 6146, 6710, 29901, 323, 9509, 6710, 13, 13, 10454, 12312, 1906, 3838, 297, 10432, 297, 285, 4615]\\n',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': \"In german \\n\\nApple: Apfel\\nBananas: Bananen\\nTelephone: Telefon\\n\\nNow, let's try means \\n\\nApple: Pomme\\nBananas: Bananes\\nTelephone: Tlphone\\n\\nNow repeat those words in Spanish in french\",\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 2,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 0,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': None,\n",
       "     'logs': '',\n",
       "     'millis': 0}]},\n",
       "  {'id': 'cmpl-cd4cf2c1c6174678a0fe24d3fb6fd639',\n",
       "   'object': 'text_completion',\n",
       "   'created': 1701293439,\n",
       "   'model': 'codellama/CodeLlama-13b-Instruct-hf',\n",
       "   'choices': [{'index': 1,\n",
       "     'text': '',\n",
       "     'logprobs': None,\n",
       "     'finish_reason': 'stop',\n",
       "     'logs': '',\n",
       "     'millis': 0}]}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn0 = endpoint.append(\"Please remember the following words: Apple, Bananas, and Telephone\\n\")\n",
    "\n",
    "pn1 = pn0.append(\"\\nNow repeat those words in French: \") \\\n",
    "    .gen(max_tokens=30, set_var=\"french\")\n",
    "\n",
    "pn2 = pn0.append(\"\\nNow repeat those words in German: \") \\\n",
    "    .gen(max_tokens=30, set_var=\"german\")\n",
    "\n",
    "pn3 = pn0.wait([\"french\", \"german\"]) \\\n",
    "    .append(\"\\nIn german {{german}} means {{french}} in french\")\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "endpoint.runAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn0 = endpoint.append(\"Please remember the following words: Apple, Bananas, and Telephone\\n\")\n",
    "\n",
    "pn1 = pn0.append(\"\\nNow repeat those words in French: \") \\\n",
    "    .gen(max_tokens=30, set_var=\"french\")\n",
    "\n",
    "pn2 = pn0.append(\"\\nNow repeat those words in German: \") \\\n",
    "    .gen(max_tokens=30, set_var=\"german\")\n",
    "\n",
    "pn3 = pn0.wait([\"french\", \"german\"]) \\\n",
    "    .append(\"\\nIn german {{german}} means {{french}} in french\")\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "endpoint.runAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn0 = endpoint.append(\"Please remember the following words: Apple, Bananas, and Telephone\\n\")\n",
    "\n",
    "pn1 = pn0.append(\"\\nNow repeat those words in French: \") \\\n",
    "    .gen(max_tokens=30, set_var=\"french\")\n",
    "\n",
    "pn2 = pn1.append(\"\\nNow repeat those words in German: \", following=pn0.id) \\\n",
    "    .gen(max_tokens=30, set_var=\"german\")\n",
    "\n",
    "pn3 = pn2.append(\"\\nIn german {{german}} means {{french}} in french\", following=pn0.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload module... 3185kB -> 9853kB id:f46c460b\n",
      "{'steps': [{'Fixed': {'text': 'Please remember the following items:', 'expand_vars': True, 'tag': '_2', 'label': '_2'}}, {'Fixed': {'text': '\\nApple', 'expand_vars': True, 'tag': '_3', 'label': '_3'}}, {'Fixed': {'text': '\\nCherries', 'expand_vars': True, 'tag': '_4', 'label': '_4'}}, {'Fixed': {'text': '\\nTomatoes', 'expand_vars': True, 'tag': '_5', 'label': '_5'}}, {'Fixed': {'text': '\\nCucumber', 'expand_vars': True, 'tag': '_6', 'label': '_6'}}, {'Fixed': {'text': '\\nStrawberries', 'expand_vars': True, 'tag': '_7', 'label': '_7'}}, {'Fixed': {'text': '\\nPears', 'expand_vars': True, 'tag': '_8', 'label': '_8'}}, {'Fixed': {'text': '\\nSquash', 'expand_vars': True, 'tag': '_9', 'label': '_9'}}, {'Fixed': {'text': '\\n\\nOk now please repeat the list:\\n', 'expand_vars': True, 'tag': '_10', 'label': '_10'}}, {'Fork': {'branches': [[{'Fixed': {'following': '_5', 'text': '\\nStrawberries\\nPears\\n\\nOk now please repeat the list:\\n', 'expand_vars': True}}, {'Gen': {'max_tokens': 30, 'tag': '_2', 'label': '_2', 'stop_at': '\\n\\n', 'set_var': 'gen_var_2'}}], [{'Wait': {'vars': ['gen_var_2']}}, {'Fixed': {'text': '{{gen_var_2}}', 'expand_vars': True}}]]}}]}\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidChunkLength\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1073\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1074\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1008\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m \u001b[39mraise\u001b[39;00m InvalidChunkLength(\u001b[39mself\u001b[39m, line) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidChunkLength\u001b[0m: InvalidChunkLength(got length b'', 0 bytes read)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1061\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[39mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[1;32m   1057\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[0;32m-> 1061\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m   1062\u001b[0m     \u001b[39m# Don't bother reading the body of a HEAD request.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response \u001b[39mand\u001b[39;00m is_response_to_head(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response):\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:727\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mexcept\u001b[39;00m (HTTPException, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    726\u001b[0m     \u001b[39m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnection broken: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[39m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m aici_steps \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39mbuild_tree_plan()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(aici_steps)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f61696369222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f656d72656b2f666173742d70726f6a6563742f616963692f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d@ssh-remote%2Btunnel/workspaces/aici/promptlib/notebooks/aici_server_example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m result \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39;49mrun(pn)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoint.py:17\u001b[0m, in \u001b[0;36mEndpointNode.run\u001b[0;34m(self, promptNode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, promptNode:PromptNode):\n\u001b[1;32m     16\u001b[0m     plan \u001b[39m=\u001b[39m promptNode\u001b[39m.\u001b[39mbuild_linear_plan()\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint\u001b[39m.\u001b[39;49mrun(plan)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoints/aici.py:27\u001b[0m, in \u001b[0;36mAICI.run\u001b[0;34m(self, prompt_plan)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, prompt_plan):\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m _submit_program(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_url, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwasm_runner_id, prompt_plan, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/workspaces/aici/promptlib/promptlib/endpoints/aici.py:80\u001b[0m, in \u001b[0;36m_submit_program\u001b[0;34m(base_url, aici_module, aici_arg, temperature, max_tokens, n, log)\u001b[0m\n\u001b[1;32m     78\u001b[0m full_resp \u001b[39m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m texts \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m n\n\u001b[0;32m---> 80\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39miter_lines():\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m line:\n\u001b[1;32m     82\u001b[0m         decoded_line: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, decode_unicode\u001b[39m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m pending \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39m pending \u001b[39m+\u001b[39m chunk\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    819\u001b[0m \u001b[39mexcept\u001b[39;00m DecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))"
     ]
    }
   ],
   "source": [
    "ep = AICI(base_url=\"http://127.0.0.1:8080/v1/\", wasm_runner_path=\"/workspaces/aici/aici_ast_runner/target/opt.wasm\")\n",
    "\n",
    "endpoint = PromptNode().set_endpoint(ep)\n",
    "\n",
    "pn = endpoint.append(\"Please remember the following items:\") \\\n",
    "    .append(\"\\nApple\") \\\n",
    "    .append(\"\\nCherries\") \\\n",
    "    .append(\"\\nTomatoes\", attrs=[\"veg\"]) \\\n",
    "    .append(\"\\nCucumber\", attrs=[\"veg\"]) \\\n",
    "    .append(\"\\nStrawberries\") \\\n",
    "    .append(\"\\nPears\") \\\n",
    "    .append(\"\\nSquash\", attrs=[\"veg\"]) \\\n",
    "    .append(\"\\n\\nOk now please repeat the list:\\n\") \\\n",
    "    .gen(max_tokens=30, stop_at=\"\\n\\n\", ignore=[\"veg\"]) # Ignore fruits that pretend to be vegetables\n",
    "\n",
    "aici_steps = endpoint.build_tree_plan()\n",
    "print(aici_steps)\n",
    "\n",
    "result = endpoint.run(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please remember the following items:\n",
      "Apple\n",
      "Cherries\n",
      "Tomatoes\n",
      "Cucumber\n",
      "Strawberries\n",
      "Pears\n",
      "Squash\n",
      "\n",
      "Ok now please repeat the list:\n",
      "\n",
      "Strawberries\n",
      "Pears\n",
      "\n",
      "Ok now please repeat the list:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': '',\n",
       " 'prompt': '',\n",
       " 'max_tokens': 200,\n",
       " 'n': 1,\n",
       " 'temperature': 0,\n",
       " 'stream': True,\n",
       " 'aici_module': 'f46c460b5c4a5de00e4d26ad04a421136d2b4257334b046a826d1e1dccad5236',\n",
       " 'aici_arg': {'steps': [{'Fixed': {'text': 'Please remember the following items:',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_66',\n",
       "     'label': '_66'}},\n",
       "   {'Fixed': {'text': '\\nApple',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_67',\n",
       "     'label': '_67'}},\n",
       "   {'Fixed': {'text': '\\nCherries',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_68',\n",
       "     'label': '_68'}},\n",
       "   {'Fixed': {'text': '\\nTomatoes',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_69',\n",
       "     'label': '_69'}},\n",
       "   {'Fixed': {'text': '\\nCucumber',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_70',\n",
       "     'label': '_70'}},\n",
       "   {'Fixed': {'text': '\\nStrawberries',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_71',\n",
       "     'label': '_71'}},\n",
       "   {'Fixed': {'text': '\\nPears',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_72',\n",
       "     'label': '_72'}},\n",
       "   {'Fixed': {'text': '\\nSquash',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_73',\n",
       "     'label': '_73'}},\n",
       "   {'Fixed': {'text': '\\n\\nOk now please repeat the list:\\n',\n",
       "     'expand_vars': True,\n",
       "     'tag': '_74',\n",
       "     'label': '_74'}},\n",
       "   {'Fork': {'branches': [[{'Fixed': {'follows': '_69',\n",
       "         'text': '\\nStrawberries\\nPears\\n\\nOk now please repeat the list:\\n',\n",
       "         'expand_vars': True}},\n",
       "       {'Gen': {'max_tokens': 30,\n",
       "         'tag': '_11',\n",
       "         'label': '_11',\n",
       "         'stop_at': '\\n\\n',\n",
       "         'set_var': 'gen_var_11'}}],\n",
       "      [{'Wait': {'vars': ['gen_var_11']}},\n",
       "       {'Fixed': {'text': '{{gen_var_11}}', 'expand_vars': True}}]]}}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
